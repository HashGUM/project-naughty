#include "llama_inference.h"
#include <godot_cpp/core/class_db.hpp>
#include <godot_cpp/variant/utility_functions.hpp>
#include <godot_cpp/classes/project_settings.hpp>
#include "../include/llama.h"
#include <fstream>
#include <sstream>

using namespace godot;

LlamaInference::LlamaInference() {
    model = nullptr;
    context = nullptr;
    sampler = nullptr;
    initialized = false;
    is_busy = false;
    
    // GPU/CPUçŠ¶æ€
    use_gpu = false;
    gpu_available = false;
    gpu_layers = -1;  // é»˜è®¤å…¨éƒ¨å±‚å¸è½½åˆ°GPU
    
    // é»˜è®¤å‚æ•°
    context_size = 2048;
    max_tokens = 256;
    temperature = 0.7f;
    threads = 4;
    
    // è‡ªå®šä¹‰é…ç½®
    custom_system_prompt = "";
    custom_grammar_content = "";
    use_custom_config = false;
    
    // åˆå§‹åŒ–llama backend
    llama_backend_init();
}

LlamaInference::~LlamaInference() {
    unload_model();
    llama_backend_free();
}

void LlamaInference::_bind_methods() {
    // ç»‘å®šæ–¹æ³•
    ClassDB::bind_method(D_METHOD("load_model", "path"), &LlamaInference::load_model);
    ClassDB::bind_method(D_METHOD("infer", "prompt"), &LlamaInference::infer);
    ClassDB::bind_method(D_METHOD("is_model_loaded"), &LlamaInference::is_model_loaded);
    ClassDB::bind_method(D_METHOD("is_inferring"), &LlamaInference::is_inferring);
    ClassDB::bind_method(D_METHOD("unload_model"), &LlamaInference::unload_model);
    
    // é…ç½®å±æ€§
    ClassDB::bind_method(D_METHOD("set_context_size", "size"), &LlamaInference::set_context_size);
    ClassDB::bind_method(D_METHOD("get_context_size"), &LlamaInference::get_context_size);
    ClassDB::add_property("LlamaInference", PropertyInfo(Variant::INT, "context_size"), "set_context_size", "get_context_size");
    
    ClassDB::bind_method(D_METHOD("set_max_tokens", "tokens"), &LlamaInference::set_max_tokens);
    ClassDB::bind_method(D_METHOD("get_max_tokens"), &LlamaInference::get_max_tokens);
    ClassDB::add_property("LlamaInference", PropertyInfo(Variant::INT, "max_tokens"), "set_max_tokens", "get_max_tokens");
    
    ClassDB::bind_method(D_METHOD("set_temperature", "temp"), &LlamaInference::set_temperature);
    ClassDB::bind_method(D_METHOD("get_temperature"), &LlamaInference::get_temperature);
    ClassDB::add_property("LlamaInference", PropertyInfo(Variant::FLOAT, "temperature"), "set_temperature", "get_temperature");
    
    ClassDB::bind_method(D_METHOD("set_threads", "count"), &LlamaInference::set_threads);
    ClassDB::bind_method(D_METHOD("get_threads"), &LlamaInference::get_threads);
    ClassDB::add_property("LlamaInference", PropertyInfo(Variant::INT, "threads"), "set_threads", "get_threads");
    
    // GPUç›¸å…³æ–¹æ³•
    ClassDB::bind_method(D_METHOD("set_gpu_layers", "layers"), &LlamaInference::set_gpu_layers);
    ClassDB::bind_method(D_METHOD("get_gpu_layers"), &LlamaInference::get_gpu_layers);
    ClassDB::add_property("LlamaInference", PropertyInfo(Variant::INT, "gpu_layers"), "set_gpu_layers", "get_gpu_layers");
    
    ClassDB::bind_method(D_METHOD("is_using_gpu"), &LlamaInference::is_using_gpu);
    ClassDB::bind_method(D_METHOD("get_device_info"), &LlamaInference::get_device_info);
    
    // ä¿¡å·
    ADD_SIGNAL(MethodInfo("inference_completed", PropertyInfo(Variant::STRING, "result")));
    ADD_SIGNAL(MethodInfo("inference_failed", PropertyInfo(Variant::STRING, "error")));
    
    // è‡ªå®šä¹‰é…ç½®æ–¹æ³•ï¼ˆæ··åˆæ¨¡å¼ï¼‰
    ClassDB::bind_method(D_METHOD("set_system_prompt", "prompt"), &LlamaInference::set_system_prompt);
    ClassDB::bind_method(D_METHOD("get_system_prompt"), &LlamaInference::get_system_prompt);
    
    ClassDB::bind_method(D_METHOD("set_grammar_content", "grammar"), &LlamaInference::set_grammar_content);
    ClassDB::bind_method(D_METHOD("get_grammar_content"), &LlamaInference::get_grammar_content);
    
    ClassDB::bind_method(D_METHOD("clear_custom_config"), &LlamaInference::clear_custom_config);
}

// GPUæ£€æµ‹
bool LlamaInference::detect_gpu() {
    // å°è¯•æ£€æµ‹CUDAè®¾å¤‡
    // æ³¨æ„ï¼šllama.cppå½“å‰ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰ç›´æ¥çš„GPUæ£€æµ‹å‡½æ•°
    // æˆ‘ä»¬é€šè¿‡å°è¯•åŠ è½½æ¨¡å‹åˆ°GPUæ¥æ£€æµ‹
    return true;  // å‡è®¾å¯ç”¨ï¼Œå®é™…åœ¨try_load_gpuä¸­éªŒè¯
}

// åŠ è½½grammarå†…å®¹ï¼ˆæ··åˆæ¨¡å¼ï¼‰
std::string LlamaInference::load_grammar_content() {
    // 1. å¦‚æœè®¾ç½®äº†è‡ªå®šä¹‰grammarï¼Œä¼˜å…ˆä½¿ç”¨
    if (!custom_grammar_content.is_empty()) {
        UtilityFunctions::print(String::utf8("âœ“ ä½¿ç”¨è‡ªå®šä¹‰Grammaré…ç½®"));
        return std::string(custom_grammar_content.utf8().get_data());
    }
    
    // 2. å¦åˆ™ä»é»˜è®¤æ–‡ä»¶è¯»å–
    String grammar_path = ProjectSettings::get_singleton()->globalize_path("res://prompt/cat_response.gbnf");
    std::string grammar_path_str = grammar_path.utf8().get_data();
    
    std::ifstream grammar_file(grammar_path_str);
    if (grammar_file.is_open()) {
        std::stringstream buffer;
        buffer << grammar_file.rdbuf();
        std::string content = buffer.str();
        grammar_file.close();
        
        UtilityFunctions::print(String::utf8("âœ“ Grammaræ–‡ä»¶åŠ è½½æˆåŠŸ: ") + grammar_path);
        return content;
    }
    
    UtilityFunctions::push_warning(String::utf8("âš  æ— æ³•è¯»å–Grammaræ–‡ä»¶: ") + grammar_path);
    return "";
}

// GPUæ¨¡å‹åŠ è½½
bool LlamaInference::try_load_gpu(const char* path) {
    llama_model_params model_params = llama_model_default_params();
    model_params.n_gpu_layers = gpu_layers;  // å¸è½½å±‚æ•°åˆ°GPU
    
    model = llama_load_model_from_file(path, model_params);
    if (!model) {
        UtilityFunctions::push_warning("GPU model loading failed");
        return false;
    }
    
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = context_size;
    ctx_params.n_threads = threads;
    ctx_params.n_batch = 512;
    
    context = llama_new_context_with_model(model, ctx_params);
    if (!context) {
        llama_free_model(model);
        model = nullptr;
        UtilityFunctions::push_warning("GPU context creation failed");
        return false;
    }
    
    // åˆ›å»ºé‡‡æ ·å™¨é“¾
    sampler = llama_sampler_chain_init(llama_sampler_chain_default_params());
    
    // 1. å…ˆæ·»åŠ å¸¸è§„é‡‡æ ·å™¨ï¼ˆé‡è¦ï¼šå¿…é¡»åœ¨grammarä¹‹å‰ï¼‰
    llama_sampler_chain_add(sampler, llama_sampler_init_temp(temperature));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(40));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.9, 1));
    
    // 2. å†æ·»åŠ Grammarçº¦æŸï¼ˆæ··åˆæ¨¡å¼ï¼šè‡ªå®šä¹‰æˆ–æ–‡ä»¶ï¼‰
    std::string json_grammar = load_grammar_content();
    
    if (!json_grammar.empty()) {
        // åˆå§‹åŒ–grammar sampler
        const llama_vocab* vocab = llama_model_get_vocab(model);
        struct llama_sampler* grammar_sampler = llama_sampler_init_grammar(vocab, json_grammar.c_str(), "root");
        
        if (grammar_sampler) {
            llama_sampler_chain_add(sampler, grammar_sampler);
            UtilityFunctions::print(String::utf8("âœ“ JSON Grammarçº¦æŸå·²å¯ç”¨"));
        } else {
            UtilityFunctions::push_warning(String::utf8("âš  JSON Grammaråˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨æ ¼å¼çº¦æŸ"));
        }
    } else {
        UtilityFunctions::push_warning(String::utf8("âš  Grammarå†…å®¹ä¸ºç©ºï¼Œå°†ä¸ä½¿ç”¨æ ¼å¼çº¦æŸ"));
    }
    
    // 3. æœ€åæ·»åŠ åˆ†å¸ƒé‡‡æ ·å™¨
    llama_sampler_chain_add(sampler, llama_sampler_init_dist(0));
    
    UtilityFunctions::print(String::utf8("âœ“ GPUæ¨¡å‹åŠ è½½æˆåŠŸ"));
    return true;
}

// CPUæ¨¡å‹åŠ è½½
bool LlamaInference::try_load_cpu(const char* path) {
    llama_model_params model_params = llama_model_default_params();
    model_params.n_gpu_layers = 0;  // å¼ºåˆ¶CPU
    
    model = llama_load_model_from_file(path, model_params);
    if (!model) {
        UtilityFunctions::push_error("CPU model loading failed");
        return false;
    }
    
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = context_size;
    ctx_params.n_threads = threads;
    ctx_params.n_batch = 512;
    
    context = llama_new_context_with_model(model, ctx_params);
    if (!context) {
        llama_free_model(model);
        model = nullptr;
        UtilityFunctions::push_error("CPU context creation failed");
        return false;
    }
    
    // åˆ›å»ºé‡‡æ ·å™¨é“¾
    sampler = llama_sampler_chain_init(llama_sampler_chain_default_params());
    
    // 1. å…ˆæ·»åŠ å¸¸è§„é‡‡æ ·å™¨ï¼ˆé‡è¦ï¼šå¿…é¡»åœ¨grammarä¹‹å‰ï¼‰
    llama_sampler_chain_add(sampler, llama_sampler_init_temp(temperature));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(40));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.9, 1));
    
    // 2. å†æ·»åŠ Grammarçº¦æŸï¼ˆæ··åˆæ¨¡å¼ï¼šè‡ªå®šä¹‰æˆ–æ–‡ä»¶ï¼‰
    std::string json_grammar = load_grammar_content();
    
    if (!json_grammar.empty()) {
        // åˆå§‹åŒ–grammar sampler
        const llama_vocab* vocab = llama_model_get_vocab(model);
        struct llama_sampler* grammar_sampler = llama_sampler_init_grammar(vocab, json_grammar.c_str(), "root");
        
        if (grammar_sampler) {
            llama_sampler_chain_add(sampler, grammar_sampler);
            UtilityFunctions::print(String::utf8("âœ“ JSON Grammarçº¦æŸå·²å¯ç”¨"));
        } else {
            UtilityFunctions::push_warning(String::utf8("âš  JSON Grammaråˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨æ ¼å¼çº¦æŸ"));
        }
    } else {
        UtilityFunctions::push_warning(String::utf8("âš  Grammarå†…å®¹ä¸ºç©ºï¼Œå°†ä¸ä½¿ç”¨æ ¼å¼çº¦æŸ"));
    }
    
    // 3. æœ€åæ·»åŠ åˆ†å¸ƒé‡‡æ ·å™¨
    llama_sampler_chain_add(sampler, llama_sampler_init_dist(0));
    
    UtilityFunctions::print(String::utf8("âœ“ CPUæ¨¡å‹åŠ è½½æˆåŠŸ"));
    return true;
}

bool LlamaInference::load_model(const String& path) {
    if (initialized) {
        UtilityFunctions::push_warning("Model already loaded");
        return false;
    }
    
    model_path = path;
    
    // è½¬æ¢è·¯å¾„
    String abs_path = ProjectSettings::get_singleton()->globalize_path(path);
    std::string path_str = abs_path.utf8().get_data();
    
    UtilityFunctions::print(String::utf8("LlamaInference: åŠ è½½æ¨¡å‹ - ") + abs_path);
    
    // 1. å°è¯•GPUåŠ è½½ï¼ˆå¦‚æœgpu_layers > 0ï¼‰
    if (gpu_layers != 0) {
        gpu_available = detect_gpu();
        if (gpu_available) {
            UtilityFunctions::print(String::utf8("å°è¯•GPUæ¨ç†..."));
            if (try_load_gpu(path_str.c_str())) {
                use_gpu = true;
                initialized = true;
                return true;
            }
            UtilityFunctions::push_warning(String::utf8("GPUåŠ è½½å¤±è´¥ï¼Œé™çº§åˆ°CPU"));
        }
    }
    
    // 2. é™çº§åˆ°CPUæˆ–ç”¨æˆ·å¼ºåˆ¶CPUæ¨¡å¼
    UtilityFunctions::print(String::utf8("ä½¿ç”¨CPUæ¨ç†..."));
    if (try_load_cpu(path_str.c_str())) {
        use_gpu = false;
        initialized = true;
        return true;
    }
    
    UtilityFunctions::push_error("Model loading failed");
    return false;
}

// æ–‡æœ¬ç”Ÿæˆæ ¸å¿ƒå‡½æ•°
std::string LlamaInference::generate_text(const std::string& prompt) {
    // è·å–vocab
    const llama_vocab* vocab = llama_model_get_vocab(model);
    
    // TokenåŒ–è¾“å…¥
    std::vector<llama_token> tokens;
    tokens.resize(prompt.size() + 1);
    int n_tokens = llama_tokenize(vocab, prompt.c_str(), prompt.size(), tokens.data(), tokens.size(), true, false);
    tokens.resize(n_tokens);
    
    if (n_tokens < 0) {
        UtilityFunctions::push_error(String::utf8("TokenåŒ–å¤±è´¥"));
        return "";
    }
    
    UtilityFunctions::print(String::utf8("TokenåŒ–æˆåŠŸï¼Œtokenæ•°: ") + String::num_int64(n_tokens));
    
    // åˆ›å»ºæ‰¹å¤„ç†
    llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());
    
    // è¯„ä¼°prompt
    if (llama_decode(context, batch) != 0) {
        UtilityFunctions::push_error(String::utf8("æ¨¡å‹è§£ç å¤±è´¥"));
        return "";
    }
    
    UtilityFunctions::print(String::utf8("Promptè¯„ä¼°å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆ..."));
    
    // ç”Ÿæˆå¾ªç¯
    std::string result;
    int n_cur = tokens.size();
    int n_gen = 0;
    
    // åœæ­¢ç¬¦ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
    const std::vector<std::string> stop_strings = {
        "\nç”¨æˆ·:",
        "\nUser:",
        "\n\n",
        "</s>",
        "<|endoftext|>"
    };
    
    while (n_gen < max_tokens) {
        // é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        llama_token new_token = llama_sampler_sample(sampler, context, -1);
        
        // ğŸ” å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯EOG tokenï¼ˆåœ¨è§£ç ä¹‹å‰ï¼‰
        if (llama_token_is_eog(vocab, new_token)) {
            UtilityFunctions::print(String::utf8("é‡åˆ°EOG token (ID: ") + String::num_int64(new_token) + String::utf8(")ï¼Œåœæ­¢ç”Ÿæˆ"));
            break;
        }
        
        // è§£ç tokenä¸ºæ–‡æœ¬
        char buf[256];
        int n = llama_token_to_piece(vocab, new_token, buf, sizeof(buf), 0, false);
        if (n > 0) {
            result.append(buf, n);
        }
        
        // æ£€æŸ¥æ˜¯å¦åŒ…å«åœæ­¢ç¬¦ï¼ˆå­—ç¬¦ä¸²å±‚é¢æ£€æŸ¥ï¼‰
        bool should_stop = false;
        for (const auto& stop_str : stop_strings) {
            size_t pos = result.find(stop_str);
            if (pos != std::string::npos) {
                // æ‰¾åˆ°åœæ­¢ç¬¦ï¼Œç§»é™¤å®ƒå¹¶åœæ­¢ç”Ÿæˆ
                UtilityFunctions::print(String::utf8("é‡åˆ°åœæ­¢ç¬¦: ") + String::utf8(stop_str.c_str()));
                result = result.substr(0, pos);
                should_stop = true;
                break;
            }
        }
        
        if (should_stop) {
            break;
        }
        
        // ç»§ç»­ç”Ÿæˆ
        batch = llama_batch_get_one(&new_token, 1);
        if (llama_decode(context, batch) != 0) {
            UtilityFunctions::push_error(String::utf8("ç”Ÿæˆè¿‡ç¨‹ä¸­è§£ç å¤±è´¥"));
            break;
        }
        
        n_gen++;
    }
    
    UtilityFunctions::print(String::utf8("ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆäº† ") + String::num_int64(n_gen) + String::utf8(" ä¸ªtoken"));
    UtilityFunctions::print(String::utf8("åŸå§‹ç”Ÿæˆå†…å®¹: [") + String::utf8(result.c_str()) + String::utf8("]"));
    UtilityFunctions::print(String::utf8("å†…å®¹é•¿åº¦: ") + String::num_int64(result.length()));
    
    return result;
}

String LlamaInference::infer(const String& prompt) {
    if (!initialized) {
        UtilityFunctions::push_error("Model not loaded");
        emit_signal("inference_failed", String::utf8("æ¨¡å‹æœªåŠ è½½"));
        return "";
    }
    
    if (is_busy) {
        UtilityFunctions::push_warning("Inference already in progress");
        return "";
    }
    
    is_busy = true;
    
    // è½¬æ¢prompt
    std::string prompt_str = prompt.utf8().get_data();
    
    // è·å–system promptï¼ˆæ··åˆæ¨¡å¼ï¼‰
    std::string system_prompt;
    
    if (!custom_system_prompt.is_empty()) {
        // ä½¿ç”¨è‡ªå®šä¹‰system prompt
        system_prompt = std::string(custom_system_prompt.utf8().get_data());
        UtilityFunctions::print(String::utf8("âœ“ ä½¿ç”¨è‡ªå®šä¹‰System Prompt"));
    } else {
        // ä»æ–‡ä»¶è¯»å–system prompt
        String system_prompt_path = ProjectSettings::get_singleton()->globalize_path("res://prompt/system_prompt.txt");
        std::string system_prompt_str = system_prompt_path.utf8().get_data();
        
        std::ifstream system_file(system_prompt_str);
        system_prompt = "ä½ æ˜¯ä¸€åªçŒ«ã€‚è¯·ç”¨JSONæ ¼å¼å›å¤ã€‚";  // é»˜è®¤å€¼
        
        if (system_file.is_open()) {
            std::stringstream buffer;
            buffer << system_file.rdbuf();
            system_prompt = buffer.str();
            system_file.close();
            UtilityFunctions::print(String::utf8("âœ“ ä»æ–‡ä»¶è¯»å–System Prompt"));
        } else {
            UtilityFunctions::push_warning(String::utf8("âš  æ— æ³•è¯»å–System Promptæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼"));
        }
    }
    
    // æ„å»ºå®Œæ•´prompt
    std::string full_prompt = system_prompt + "\nç”¨æˆ·: " + prompt_str + "\nå›ç­”:";
    
    UtilityFunctions::print(String::utf8("æ¨ç†ä¸­..."));
    UtilityFunctions::print(String::utf8("Prompté•¿åº¦: ") + String::num_int64(full_prompt.length()));
    UtilityFunctions::print(String::utf8("=== å®Œæ•´Prompt ==="));
    UtilityFunctions::print(String::utf8(full_prompt.c_str()));
    UtilityFunctions::print(String::utf8("=== Promptç»“æŸ ==="));
    
    // è°ƒç”¨llama.cppç”Ÿæˆ
    std::string result = generate_text(full_prompt);
    
    is_busy = false;
    
    // æ£€æŸ¥ç”Ÿæˆç»“æœ
    if (result.empty()) {
        UtilityFunctions::push_warning(String::utf8("æ¨¡å‹ç”Ÿæˆäº†ç©ºå›å¤"));
        String empty_msg = String::utf8("{}");
        emit_signal("inference_completed", empty_msg);
        return empty_msg;
    }
    
    // ========== æ¸…ç†ç»“æœ ==========
    std::string cleaned_result = result;
    
    // 1. ç§»é™¤æ‰€æœ‰åœæ­¢ç¬¦å’Œç‰¹æ®Šæ ‡è®°
    const std::vector<std::string> cleanup_strings = {
        "</s>",
        "<|endoftext|>",
        "<|end|>",
        "<|im_end|>",
        "<|im_start|>assistant",
        "<|im_start|>",
        "\nç”¨æˆ·:",
        "\nUser:",
        "\nåŠ©æ‰‹:",
        "\nAssistant:"
    };
    
    for (const auto& cleanup_str : cleanup_strings) {
        size_t pos = 0;
        while ((pos = cleaned_result.find(cleanup_str, pos)) != std::string::npos) {
            cleaned_result.erase(pos, cleanup_str.length());
        }
    }
    
    // 2. ç§»é™¤å¼€å¤´çš„ç©ºç™½å’Œå¤šä½™ç¬¦å·
    while (!cleaned_result.empty() && 
           (cleaned_result[0] == ' ' || cleaned_result[0] == '\n' || 
            cleaned_result[0] == '\r' || cleaned_result[0] == '\t' ||
            cleaned_result[0] == '[')) {
        cleaned_result.erase(0, 1);
    }
    
    // 3. ç§»é™¤ç»“å°¾çš„ç©ºç™½å’Œå¤šä½™ç¬¦å·
    while (!cleaned_result.empty() && 
           (cleaned_result.back() == ' ' || cleaned_result.back() == '\n' || 
            cleaned_result.back() == '\r' || cleaned_result.back() == '\t' ||
            cleaned_result.back() == ']')) {
        cleaned_result.pop_back();
    }
    
    // 4. éªŒè¯ç»“æœä¸ä¸ºç©º
    if (cleaned_result.empty()) {
        UtilityFunctions::push_warning(String::utf8("æ¸…ç†åç»“æœä¸ºç©º"));
        String empty_msg = String::utf8("{}");
        emit_signal("inference_completed", empty_msg);
        return empty_msg;
    }
    
    String godot_result = String::utf8(cleaned_result.c_str());
    emit_signal("inference_completed", godot_result);
    
    UtilityFunctions::print(String::utf8("âœ“ æ¨ç†å®Œæˆï¼Œæ¸…ç†åé•¿åº¦: ") + String::num_int64(cleaned_result.length()));
    
    return godot_result;
}

bool LlamaInference::is_model_loaded() const {
    return initialized;
}

bool LlamaInference::is_inferring() const {
    return is_busy;
}

void LlamaInference::unload_model() {
    if (!initialized) return;
    
    UtilityFunctions::print(String::utf8("LlamaInference: å¸è½½æ¨¡å‹"));
    
    // é‡Šæ”¾èµ„æº
    if (sampler) {
        llama_sampler_free(sampler);
        sampler = nullptr;
    }
    if (context) {
        llama_free(context);
        context = nullptr;
    }
    if (model) {
        llama_free_model(model);
        model = nullptr;
    }
    
    initialized = false;
    is_busy = false;
    use_gpu = false;
}

// é…ç½®æ–¹æ³•å®ç°
void LlamaInference::set_context_size(int size) {
    context_size = size;
}

int LlamaInference::get_context_size() const {
    return context_size;
}

void LlamaInference::set_max_tokens(int tokens) {
    max_tokens = tokens;
}

int LlamaInference::get_max_tokens() const {
    return max_tokens;
}

void LlamaInference::set_temperature(float temp) {
    temperature = temp;
}

float LlamaInference::get_temperature() const {
    return temperature;
}

void LlamaInference::set_threads(int count) {
    threads = count;
}

int LlamaInference::get_threads() const {
    return threads;
}

// GPUç›¸å…³æ–¹æ³•å®ç°
void LlamaInference::set_gpu_layers(int layers) {
    if (initialized) {
        UtilityFunctions::push_warning("Cannot change GPU layers while model is loaded");
        return;
    }
    gpu_layers = layers;
}

int LlamaInference::get_gpu_layers() const {
    return gpu_layers;
}

bool LlamaInference::is_using_gpu() const {
    return use_gpu;
}

String LlamaInference::get_device_info() const {
    if (!initialized) {
        return String::utf8("æ¨¡å‹æœªåŠ è½½");
    }
    
    if (use_gpu) {
        return String::utf8("GPU (") + String::num(gpu_layers) + String::utf8(" å±‚)");
    } else {
        return String::utf8("CPU (") + String::num(threads) + String::utf8(" çº¿ç¨‹)");
    }
}

// è‡ªå®šä¹‰é…ç½®æ–¹æ³•å®ç°ï¼ˆæ··åˆæ¨¡å¼ï¼‰
void LlamaInference::set_system_prompt(const String& prompt) {
    custom_system_prompt = prompt;
    use_custom_config = !prompt.is_empty() || !custom_grammar_content.is_empty();
    
    if (!prompt.is_empty()) {
        UtilityFunctions::print(String::utf8("âœ“ è‡ªå®šä¹‰System Promptå·²è®¾ç½®"));
    }
}

String LlamaInference::get_system_prompt() const {
    return custom_system_prompt;
}

void LlamaInference::set_grammar_content(const String& grammar) {
    custom_grammar_content = grammar;
    use_custom_config = !custom_system_prompt.is_empty() || !grammar.is_empty();
    
    if (!grammar.is_empty()) {
        UtilityFunctions::print(String::utf8("âœ“ è‡ªå®šä¹‰Grammarå·²è®¾ç½®"));
    }
    
    // å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œéœ€è¦æç¤ºé‡æ–°åŠ è½½
    if (initialized) {
        UtilityFunctions::push_warning(String::utf8("âš  Grammarå·²æ›´æ”¹ï¼Œéœ€è¦é‡æ–°åŠ è½½æ¨¡å‹æ‰èƒ½ç”Ÿæ•ˆ"));
    }
}

String LlamaInference::get_grammar_content() const {
    return custom_grammar_content;
}

void LlamaInference::clear_custom_config() {
    custom_system_prompt = "";
    custom_grammar_content = "";
    use_custom_config = false;
    
    UtilityFunctions::print(String::utf8("âœ“ å·²æ¸…é™¤è‡ªå®šä¹‰é…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶"));
    
    if (initialized) {
        UtilityFunctions::push_warning(String::utf8("âš  é…ç½®å·²æ¸…é™¤ï¼Œéœ€è¦é‡æ–°åŠ è½½æ¨¡å‹æ‰èƒ½ç”Ÿæ•ˆ"));
    }
}

